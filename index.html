
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="cvpr, workshop, computer vision, computer graphics, visual learning, simulation environments, robotics, machine learning, natural language processing, reinforcement learning">

  <link rel="shortcut icon" href="/static/img/ico/favicon.png">



  <title>3D Scene Generation</title>
  <meta name="description" content="Website for the Workshop on 3D Scene Generation at CVPR 2019 ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Anti-UAV"/>
  <meta property="og:url" content="https://Anti-UAV.github.io/"/>
  <meta property="og:description" content="Website for the Workshop on 3D Scene Generation at CVPR 2019 ---"/>
  <meta property="og:site_name" content="Anti-UAV"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="3D Scene Generation"/>
  <meta name="twitter:image" content="https://3dscenegen.github.io/static/img/splash.png">
  <meta name="twitter:url" content="https://3dscenegen.github.io"/>
  <meta name="twitter:description" content="Website for the Workshop on 3D Scene Generation at CVPR 2019 ---"/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="/css/main.css" media="screen,projection">
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Call for Papers</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Catch UAVs that Want to Spy You:</h1></center>
    <center><h2>Detection and Tracking of Unmanned Aerial Vehicle (UAV) in the Wild and the 1st Anti-UAV Challenge.</h2></center>
    <center>Sunday June 16 2019, 8:45am -- 5:40pm, 103A (Posters Sessions: Pacific Arena Ballroom)</center>
    <br />
    <strong><span style="color:#e74c3c;font-weight:400;">If you attended the workshop, please fill out our survey! This helps us improve future workshop offerings.</span></strong> <a href="https://forms.gle/fvcseYdQVzcoeY7u5">Click here for survey</a>
  </div>
</div>

<hr />

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="/static/img/splash.png" />
    <p> Image credit: [1, 2, 7, 12, 6, 4, 5]</p>
  </div>
</div>





<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Main Organizers</h2>
  </div>
</div>


<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Jian Zhao</td>
          <td>Assistant Professor</td>
          <td>Institute of North Electronic Equipment</td>
          <td>zhaojian90@u.nus.edu</td>
        </tr>
        <tr>
          <td>Qiang Wang</td>
          <td>Ph.D. Candidate</td>
          <td>Chinese Academy of Sciences</td>
          <td>qiang.wang@nlpr.ia.ac.cn</td>
		</tr>
        <tr>
          <td>Junliang Xing</td>
          <td>Professor</td>
          <td>Chinese Academy of Sciences</td>
          <td>jlxing@nlpr.ia.ac.cn</td>
		</tr>
        <tr>
          <td>Guibo Zhu</td>
          <td>Associate Professor</td>
		  <td>Chinese Academy of Sciences</td>
          <td>gbzhu@nlpr.ia.ac.cn</td>
        </tr>
        <tr>
          <td>Xiaopeng Hong</td>
          <td>Distinguished Research Fellow</td>
		  <td>Xi’an Jiaotong University</td>
          <td>hongxiaopeng@ieee.org</td>
        </tr>
        <tr>
          <td>Sheng Mei Shen</td>
          <td>Chief Scientist and Managing Director</td>
		  <td>Pensees Singapore</td>
          <td>jane.shen@pensees.ai</td>
        </tr>
        <tr>
          <td>Jiashi Feng</td>
          <td>Assistant Professor</td>
		  <td>National University of Singapore</td>
          <td>elefjia@nus.edu.sg</td>
        </tr>
	  </tbody>
    </table>
  </div>
</div>
<p><br /></p>


<div class="row">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The submissions are expected to deal with visual perception and processing tasks which include but are not limited to:
    </p>
    <ul>
      <li>Applications of computer vision on UAVs</li>
      <li>Strategies for searching of UAVs based on NIR and/or VIS data</li>
      <li>Spectrum sensing techniques for UAVs detection</li>
      <li>Localization and open-set identification of UAVs</li>
      <li>Scene understanding for UAVs</li>
      <li>Small/tiny object detection and tracking techniques</li>
      <li>Fine-grained object recognition</li>
      <li>Real-time deep learning inference</li>
      <li>Infrared image and video analysis</li>
      <li>Multimodal fusion techniques</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 6 pages excluding references and acknowledgements.
      The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted extended abstracts will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We also welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
      Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:3dscenegeneration@gmail.com">3dscenegeneration@gmail.com</a></span>
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
    </p>
  </div>
</div>
<p><br /></p>

<p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <h2>Background and experience that makes the proposers well suited for organizing the workshop</h2>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Jian Zhao</b> is currently an assistant professor with Institute of North Electronic Equipment, Beijing, China. He received his Ph.D. degree from National University of Singapore (NUS) in 2019 under the supervision of Assist. Prof. Jiashi Feng, Assoc. Prof. Shuicheng Yan, and Prof. Hengzhu Liu. He has published cutting-edge papers on unconstrained/large-scale/low-shot face verification/identification and human parsing as the first author (including the following conferences and journals: NIPS, CVPR, ECCV, IJCAI, AAAI, ACM MM, BMVC; T-PAMI, IJCV). He has won the Lee Hwee Kuan Award (Gold Award) on PREMIA 2019 as the first author. He has won the “Best Student Paper Award” on ACM MM 2018 as the first author. He has won the top-3 awards several times on world-wide competitions on face recognition, human parsing and pose estimation as the first author. His main research interests include deep learning, pattern recognition, computer vision and multimedia. He and his collaborators has also successfully organized the 2nd Look Into Person (LIP) workshop and challenge on CVPR 2018.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Mr. Qiang Wang</b> is currently a Ph.D. candidate in National Laboratory of Pattern Recognition at the Institute of Automation, Chinese Academic of Sciences, advised by Prof. Weiming Hu. His research interest is mainly computer vision and video object tracking, particularly single object tracking, video object segmentation. He won the VOT2018 real-time challenge. He has participated in the program committee of VOT2018 and VOT2019.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Junliang Xing</b> is currently a professor with the Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China. He received his B.E. degrees in Computer Science and Technology as well as Applied Mathematics from Xi'an Jiaotong University in 2007, and his Ph.D. degree in Computer Science and Technology from Tsinghua University, 2012. He has published over 100 papers in peer-reviewed international conferences like ICCV, CVPR, ECCV, ACM Multimedia, AAAI, IJCAI, and journals like TPAMI, IJCV, TIP, PR. He has translated two books in computer vision and wrote one book on deep learning. Dr. Xing was the recipient of Google Ph.D. Fellowship in 2011, the Best Paper Award of ACM International Conference on Multimedia in 2013, and the champions of many international AI technical competitions in face recognition, pose estimation, etc. His research areas lie in computer vision and machine learning, with a main focus on computer vision problems related to human face and body, and computer gaming problems using deep reinforcement learning models.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Guibo Zhu</b> is currently an associate professor with the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China. He received the B.E. degree from Wuhan University, Wuhan, China, in 2009, the Ph.D. degree from the University of Chinese Academy of Sciences, Beijing, China, in 2016. He has published over 10 papers as the first author in international conferences like AAAI, IJCAI, BMVC, and journals like T-IP, T-NNLS, T-Cyber, CVIU. He has won two champions of domestic AI technical competitions in single visual tracking and video understanding as the first author. His research interests include computer vision, pattern recognition, and machine learning, especially paying attention on single object tracking, object detection, video understanding and meta-learning.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Xiaopeng Hong</b> is currently a distinguished research fellow at Xi’an Jiaotong University, PRC. He had been a Docent with the Center for Machine Vision and Signal Analysis, University of Oulu, Finland, where he had been a senior researcher from 2011 to 2019. Xiaopeng has been a PI of an Infotech Oulu Postdoctoral funding project and the project manager of an Academy of Finland ICT 2023 funding project (also the only co-writer of that proposal). He has (co-)authored over 40 articles in peer-reviewed journals and conferences such as IEEE T-PAMI, IEEE T-IP, CVPR, ICCV, and IJCAI. His research about micro-expression analysis has been reported by International media like MIT Technology Review and Daily Mail. Xiaopeng has served as a reviewer for a few top-tier journals and conferences and has been ranked as an ‘Outstanding Reviewer’ for two Elsevier journals: Pattern Recognition and Neurocomputing. His current research interests include visual surveillance and micro-expression analysis, etc.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Mrs. Shengmei Shen</b>  received the Graduate and Master degrees from Xidian University, Xi’an, China. She is currently working as Chief Scientist and Managing Director at Pensees Singapore leading AI technology development especially in computer vision and deep learning for smart city including surveillance, smart community, smart manufactory, robotics and other applications with smart innovation and solution. She had been working as an assistant director in Panasonic R&D Center Singapore since 1992 with 300 patents filed, actively involved in image recognition and sensing technology development, machine vision as well as audio–visual compression for B2C and B2B business. Under her leadership and technology vision with strong business sense, she has built up several core competent areas as a strong player in AI development and deployment, to provide the technology and service to company’s products and business. 
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Jiashi Feng</b> is currently an assistant professor with the Department of Electrical and Computer Engineering at National University of Singapore. He received his Ph.D. degree from NUS in 2014. Before joining NUS, he was a postdoc researcher in the EECS department and ICSI at the University of California, Berkeley, working with Trevor Darrell. His research areas include computer vision, machine learning and deep learning. He received the best technical demo award from ACM MM 2012 and best paper award from TASK-CV ICCV 2015.
    </p>
  </div>
</div>
<p><br /></p>


<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Description of the 1st Anti-UAV Challenge:</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Recently, unmanned aerial vehicle (UAV) is growing rapidly in a wide range of consumer communications and networks with their autonomy, flexibility, and a broad range of application domains. UAV applications offer possible civil and public domain applications in which single or multiple UAVs may be used. At the same time, we also need to be aware of the potential threat to our lives caused by UAV intrusion. Earlier this year, multiple instances of drone sightings halted air traffic at airports, leading to significant economic losses for airlines.
	</p>
    <p>
      This workshop focuses on state-of-the-art anti-UAV systems in a bid to safeguard flights. Historically, radar is certainly a very mature technology for detecting traditional incoming airborne threats. However, today, these small drones are challenging for radar to accurately and reliably detect. It is due to the combination of a very small radar cross section and the erratic flight paths of these UAV drones that make trustworthy detection almost impossible. Indeed, how to use computer vision algorithms to perceive UAVs is a crucial part of the whole defense system.
    </p>
    <p>
      The current computer vision research for UAV lacks a high-quality benchmark in dynamic environments. To mitigate this gap, this workshop presents a benchmark dataset and evaluation methodology for the area of detecting and tracking UAVs. The dataset consists of sixty high quality, Full HD video sequences (both RGB and Thermal Infrared), spanning multiple occurrences of multi-scale UAVs. This workshop also encourages participants to establish approaches to fully automatic detection and tracking of UAVs in videos. Although solutions can be derived from the off-the-shelf detection and tracking algorithms, the training and customizations of them for UAVs pose unique challenges. Specifically, how to detect and track fast-moving drones in noisy (e.g., occlusion by cloud and buildings, and fake targets like kites, balloons, birds, etc.) environments and how to use infrared images to locate UAVs in total darkness are not explored enough.
    </p>
    <p>
      This workshop will bring together academic and industrial experts in the field of UAVs to discuss the techniques and applications of tracking UAVs. Participants are invited to submit their original contributions, surveys, and case studies that address the works of UAV’s detection and tracking issues.
    </p>
  </div>
</div>
<p><br /></p>



<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome and Introduction</td>
          <td>8:45am - 9:00am</td>
        </tr>
        <tr>
          <td>Invited Talk 1: Daniel Aliaga</td>
          <td>9:00am - 9:25am</td>
        </tr>
        <tr>
          <td>Invited Talk 2: Angela Dai -- "From unstructured range scans to 3d models"</td>
          <td>9:25am - 9:50am</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>9:50am - 10:05am</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session (Pacific Arena Ballroom, #24-#33)</td>
          <td>10:05am - 11:00am</td>
        </tr>
        <tr>
          <td>Invited Talk 3: Johannes L. Schönberger -- "3D Scene Reconstruction from Unstructured Imagery"</td>
          <td>11:00am - 11:25am</td>
        </tr>
        <tr>
          <td>Invited Talk 4: Vladlen Koltun</td>
          <td>11:25am - 11:50am</td>
        </tr>
        <tr>
          <td>Lunch Break</td>
          <td>11:50am - 1:00pm</td>
        </tr>
        <tr>
          <td>Industry Participant Talks</td>
          <td>1:00pm - 2:40pm</td>
        </tr>
        <tr>
          <td>Invited Talk 5: Ellie Pavlick -- "Natural Language Understanding: Where we are stuck and where you can help"</td>
          <td>2:40pm - 3:05pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session (Pacific Arena Ballroom, #24-#33)</td>
          <td>3:05pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Talk 6: Jiajun Wu</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Invited Talk 7: Kristen Grauman -- "Learning to explore 3D scenes"</td>
          <td>4:25pm - 4:50pm</td>
        </tr>
        <tr>
          <td>Invited Talk 8: Siddhartha Chaudhuri -- "Recursive neural networks for scene synthesis"</td>
          <td>4:50pm - 5:15pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>5:15pm - 6:00pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</span><br />
    Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove<br />
    <a href="https://drive.google.com/file/d/1oYUQOicJ-gIcLaQUnrE23azPDMW5VphP/view?usp=sharing">Paper</a> | Poster #24 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">DeepPerimeter: Indoor Boundary Estimation from Posed Monocular Sequences</span><br />
    Ameya Phalak, Zhao Chen, Darvin Yi, Khushi Gupta, Vijay Badrinarayanan, Andrew Rabinovich<br />
    <a href="https://drive.google.com/file/d/1fGI5oRWTg2Bu5the1rNONbrS9BQX00bA/view?usp=sharing">Paper</a> | Poster #25 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Learning a Generative Model for Multi-Step Human-Object Interactions from
      Videos</span><br />
    He Wang, Sören Pirk, Ersin Yumer, Vladimir G. Kim, Ozan Sener, Srinath Sridhar, Leonidas J. Guibas<br />
    <a href="https://drive.google.com/file/d/1fGI5oRWTg2Bu5the1rNONbrS9BQX00bA/view?usp=sharing">Paper</a> | Poster #26 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Scenic: A Language for Scenario Specification and Scene Generation</span><br />
    Daniel J. Fremont, Xiangyu Yue, Tommaso Dreossi, Shromona Ghosh, Alberto L. Sangiovanni-Vincentelli, Sanjit A.
    Seshia<br />
    <a href="https://drive.google.com/file/d/1pNx5pVv9lezequbXwuT2A0ZNTDblWzjd/view?usp=sharing">Paper</a> | Poster #27 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data
      Augmentation</span><br />
    Cheng Sun, Chi-Wei Hsiao, Min Sun, Hwann-Tzong Chen<br />
    <a href="https://drive.google.com/file/d/1NmylUpsThqBxK6MmgVXgsovwmFARynuX/view?usp=sharing">Paper</a> | Poster #28 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Towards Training Person Detectors from Synthetic RGB-D Data</span><br />
    Timm Linder, Michael Johan Hernandez Leon, Narunas Vaskevicius, Kai O. Arras<br />
    <a href="https://drive.google.com/file/d/1OnufiJsY5-H0jjaaCDE99VMgDWg1RH1K/view?usp=sharing">Paper</a> | Poster #29 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">HomeNet: Layout Generation of Indoor Scenes from Panoramic Images Using Pyramid
      Pooling</span><br />
    Krisha Mehta, Yash Kotadia<br />
    <a href="https://drive.google.com/file/d/1KYcuWqcSxQ_PeK5FOnjS40cKc9wALjak/view?usp=sharing">Paper</a>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Learning to Encode Spatial Relations from Natural Language</span><br />
    Tiago Ramalho, Tomas Kocisky, Frederic Besse, S. M. Ali Eslami, Gabor Melis, Fabio Viola, Phil Blunsom, Karl Moritz
    Hermann<br />
    <a href="https://drive.google.com/file/d/1bSFiCkqgNM438uNOcdBHrMuM-TtpNpfo/view?usp=sharing">Paper</a> | Poster #30 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments</span><br />
    Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, and Jan Kautz<br />
    <a href="https://drive.google.com/file/d/11qXyTl2mgo07pWLsdHQRLLs3J9S2C7MS/view?usp=sharing">Paper</a> | Poster #31 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">The RobotriX: A Large-scale Dataset of Embodied Robots in Virtual Reality</span><br />
    Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John A. Castro-Vargas, Sergio Orts-Escolano, Alvaro
    Jover-Alvarez, Jose Garcia-Rodriguez<br />
    <a href="https://drive.google.com/file/d/1ivUa47Y1_xqByQepUz_K3K7dXeQkACqU/view?usp=sharing">Paper</a> | Poster #32 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Revealing Scenes by Inverting Structure from Motion Reconstructions</span><br />
    Francesco Pittaluga, Sanjeev J. Koppal, Sing Bing Kang, Sudipta N. Sinha<br />
    <a href="https://drive.google.com/file/d/1iEbA-7-plRImJP4-L9rQPld0yCnO5OdW/view?usp=sharing">Paper</a> | Poster #33 AM (morning)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</span><br />
    Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, Shenghua Gao<br />
    <a href="https://drive.google.com/file/d/1ojxIpBupU8cw0IOlE1G9Ky5GpMTU3Hrc/view?usp=sharing">Paper</a> | Poster #24 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image</span><br />
    Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, Jan Kautz<br />
    <a href="https://drive.google.com/file/d/1LRwKQjADtWbNpNSNWvJk7iqQzftloGyp/view?usp=sharing">Paper</a> | Poster #25 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout</span><br />
    Yifei Shi, Angel Xuan Chang, Zhelun Wu, Manolis Savva, Kai Xu<br />
    <a href="https://drive.google.com/file/d/1tbQCCOyACVGNvPVdcqHr08WXWTxU26b8/view?usp=sharing">Paper</a>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D Shapes</span><br />
    Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, Kai Xu<br />
    <a href="https://drive.google.com/file/d/1Xto0tZlYx4jAxeCVuUlLycWErysCJiTc/view?usp=sharing">Paper</a> | Poster #26 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks</span><br />
    Kuan Fang, Alexander Toshev, Li Fei-Fei, Silvio Savarese<br />
    <a href="https://drive.google.com/file/d/1S7wO9pOdaS7W_v4nm-DSjKbRmU3LX0qk/view?usp=sharing">Paper</a> | Poster #27 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D
      Object Understanding</span><br />
    Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, Hao Su<br />
    <a href="https://drive.google.com/file/d/16Z8UKKGFVjrA9V4N2MLbGBZVKR1KAWEj/view?usp=sharing">Paper</a> | Poster #28 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Learning Implicit Fields for Generative Shape Modeling</span><br />
    Zhiqin Chen, Hao Zhang<br />
    <a href="https://drive.google.com/file/d/1ufm_7J6pbVMxHTdrv__bhNK1kL5obSbc/view?usp=sharing">Paper</a> | Poster #29 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Learning 3D Human Dynamics from Video</span><br />
    Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, Jitendra Malik<br />
    <a href="https://arxiv.org/abs/1812.01601">Paper</a> | Poster #30 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">PCN: Point Completion Network</span><br />
    Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert<br />
    <a href="https://drive.google.com/file/d/1rrS3bB4QnNs4GX8ZNHHX1VkoYCaOz5G0/view?usp=sharing">Paper</a> | Poster #31 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution
      Signals on Meshes </span><br />
    Jingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser, Matthias Niessner, Leonidas Guibas<br />
    <a href="https://arxiv.org/abs/1812.00020">Paper</a> | Poster #32 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative
      Models</span><br />
    Daniel Ritchie, Kai Wang, Yu-an Lin<br />
    <a href="https://arxiv.org/abs/1811.12463">Paper</a> | Poster #33 PM (afternoon)
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="http://vladlen.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/vladlen.png" /></a>
    <p>
      <b><a href="http://vladlen.info/">Vladlen Koltun</a></b> is a Senior Principal Researcher and the director of the Intelligent Systems Lab at Intel. The lab is devoted to high-impact basic research on intelligent systems. Previously, he has been a Senior Research Scientist at Adobe Research and an Assistant Professor at Stanford where his theoretical research was recognized with the National Science Foundation (NSF) CAREER Award (2006) and the Sloan Research Fellowship (2007).
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="http://www.cs.utexas.edu/users/grauman/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/grauman.png" /></a>
    <p>
      <b><a href="http://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a></b> is a Professor in the Department of Computer Science at the University of Texas at Austin and a Research Scientist in Facebook AI Research (FAIR).  Her research in computer vision and machine learning focuses on visual recognition and search.  Before joining UT-Austin in 2007, she received her Ph.D. at MIT.  She is an Alfred P. Sloan Research Fellow and Microsoft Research New Faculty Fellow, a recipient of NSF CAREER and ONR Young Investigator awards, the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the International Joint Conference on Artificial Intelligence (IJCAI), the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013, and the Helmholtz Prize in 2017. 
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://demuc.de/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/johannes.png" /></a>
    <p>
      <b><a href="https://demuc.de/">Johannes L. Schönberger</a></b> is a Senior Scientist at the Microsoft Mixed Reality and AI lab in Zürich. He obtained his PhD in Computer Science in the Computer Vision and Geometry Group at ETH Zürich, where he was advised by Marc Pollefeys and co-advised by Jan-Michael Frahm. He received a BSc from TU Munich and an MSc from UNC Chapel Hill. In addition, he also spent time at Microsoft Research, Google, and the German Aerospace Center. His main research interests lie in robust image-based 3D modeling. More broadly, he is interested in computer vision, geometry, structure-from-motion, (multi-view) stereo, localization, optimization, machine learning, and image processing. He developed the open-source software COLMAP - an end-to-end image-based 3D reconstruction software, which achieves state-of-the-art results on recent reconstruction benchmarks.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://cs.brown.edu/people/epavlick/index.html"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/ellie.png" /></a>
    <p>
      <b><a href="https://cs.brown.edu/people/epavlick/index.html">Ellie Pavlick</a></b> is an Assistant Professor of Computer Science at Brown University, and an academic partner with Google AI. She received her PhD in Computer Science from the University of Pennsylvania. She is interested in building better computational models of natural language semantics and pragmatics: how does language work, and how can we get computers to understand it the way humans do?
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.purdue.edu/homes/aliaga/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/aliaga.png" /></a>
    <p>
      <b><a href="https://www.cs.purdue.edu/homes/aliaga/">Daniel Aliaga</a></b> does research primarily in the area of 3D computer graphics but overlaps with computer vision and visualization while also having strong multi-disciplinary collaborations outside of computer science. His research activities are divided into three groups: a) his pioneering work in the multi-disciplinary area of inverse modeling and design; b) his first-of-its-kind work in codifying information into images and surfaces, and c) his compelling work in a visual computing framework including high-quality 3D acquisition methods. Dr. Aliaga’s inverse modeling and design is particularly focused at digital city planning applications that provide innovative “what-if” design tools enabling urban stake holders from cities worldwide to automatically integrate, process, analyze, and visualize the complex interdependencies between the urban form, function, and the natural environment.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cse.iitb.ac.in/~sidch/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/sid.png" /></a>
    <p>
      <b><a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a></b> is a Senior Research Scientist at Adobe Research, and Assistant Professor (on leave) of Computer Science and Engineering at IIT Bombay. His research focuses on richer tools for designing three-dimensional objects, particularly by novice and casual users, and on related problems in 3D shape understanding, synthesis and reconstruction. He received his PhD from Stanford University, followed by a postdoc at Princeton and a year teaching at Cornell. Apart from basic research, he is also the original author of the commercial 3D modeling package Adobe Fuse.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="http://graphics.stanford.edu/~adai/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/angela.png" /></a>
    <p>
      <b><a href="http://graphics.stanford.edu/~adai/">Angela Dai</a></b> is a postdoctoral researcher at the Technical University of Munich.  She received her Ph.D. in Computer Science at Stanford University advised by Pat Hanrahan. Her research focuses on 3D reconstruction and understanding with commodity sensors. She received her Masters degree from Stanford University and her Bachelors degree from Princeton University. She is a recipient of a Stanford Graduate Fellowship.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/jiajun.png" /></a>
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is a fifth-year PhD student at MIT, advised by Bill Freeman and Josh Tenenbaum. He received his undergraduate degree from Tsinghua University, working with Zhuowen Tu. He has also spent time at research labs of Microsoft, Facebook, and Baidu. His research has been supported by fellowships from Facebook, Nvidia, Samsung, Baidu, and Adobe. He studies machine perception, reasoning, and its interaction with the physical world, drawing inspiration from human cognition.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <b>Industry Participants</b>
    <p>The workshop also features presentations by representatives of the following companies:</p>
  </div>
</div>
<div class="row">
  <div class="col-md-3">
    <a href="https://planner5d.com/"><img src="/static/img/p5d.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://wayfair.com/"><img src="/static/img/wayfair.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://modsy.com/"><img src="/static/img/modsy.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://zillow.com/"><img src="/static/img/zillow.png" /></a>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="/static/img/people/angel.png" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Eloquent Labs, Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="/static/img/people/daniel.png" />
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="/static/img/people/qixing.png" />
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="/static/img/people/manolis.png" />
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Facebook AI Research, Simon Fraser University</h6>
    </div>
  </div>
</div>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<!-- <div class="row">
  <div class="col-md-12">
  TBD
  </div>
</div> -->

<p class="paper"><span class="papertitle">[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span><br />
<span class="authors">D. Ritchie, K. Wang, and Y.a. Lin</span><br />
<span class="journal"><em>CoRR</em>, vol. arXiv:1811.12463, 2018</span></p>

<p class="paper"><span class="papertitle">[2] GRAINS: Generative Recursive Autoencoders for INdoor Scenes</span><br />
<span class="authors">M. Li, A.G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. Zhang</span><br />
<span class="journal"><em>CoRR</em>, vol. arXiv:1807.09193, 2018</span></p>

<p class="paper"><span class="papertitle">[3] Gibson env: real-world perception for embodied agents</span><br />
<span class="authors">F. Xia, A. R. Zamir, Z.Y. He, A. Sax, J. Malik, and S. Savarese</span><br />
<span class="journal">Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on, IEEE, 2018</span></p>

<p class="paper"><span class="papertitle">[4] VirtualHome: Simulating Household Activities via Programs</span><br />
<span class="authors">X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba</span><br />
<span class="journal">CVPR, 2018</span></p>

<p class="paper"><span class="papertitle">[5] Embodied Question Answering</span><br />
<span class="authors">A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra</span><br />
<span class="journal">CVPR, 2018</span></p>

<p class="paper"><span class="papertitle">[6] ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</span><br />
<span class="authors">A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner</span><br />
<span class="journal">Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2018</span></p>

<p class="paper"><span class="papertitle">[7] SeeThrough: Finding Objects in Heavily Occluded Indoor Scene Images</span><br />
<span class="authors">N. Mitra, V. Kim, E. Yumer, M. Hueting, N. Carr, and P. Reddy</span><br />
<span class="journal">2018 International Conference on 3D Vision (3DV), 2018</span></p>

<p class="paper"><span class="papertitle">[8] Matterport3D: Learning from RGB-D Data in Indoor Environments</span><br />
<span class="authors">A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang</span><br />
<span class="journal"><em>International Conference on 3D Vision (3DV)</em>, 2017</span></p>

<p class="paper"><span class="papertitle">[9] Joint 2D-3D-semantic data for indoor scene understanding</span><br />
<span class="authors">I. Armeni, S. Sax, A.R. Zamir, and S. Savarese</span><br />
<span class="journal"><em>arXiv preprint arXiv:1702.01105</em>, 2017</span></p>

<p class="paper"><span class="papertitle">[10] MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</span><br />
<span class="authors">M. Savva, A.X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun</span><br />
<span class="journal"><em>arXiv:1712.03931</em>, 2017</span></p>

<p class="paper"><span class="papertitle">[11] AI2-THOR: An interactive 3D environment for visual AI</span><br />
<span class="authors">E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi</span><br />
<span class="journal"><em>arXiv preprint arXiv:1712.05474</em>, 2017</span></p>

<p class="paper"><span class="papertitle">[12] Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</span><br />
<span class="authors">Y. Zhang, S. Song, E. Yumer, M. Savva, J.Y. Lee, H. Jin, and T. Funkhouser</span><br />
<span class="journal"><em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017</span></p>

<p class="paper"><span class="papertitle">[13] Semantic scene completion from a single depth image</span><br />
<span class="authors">S. Song, F. Yu, A. Zeng, A.X. Chang, M. Savva, and T. Funkhouser</span><br />
<span class="journal">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2017</span></p>

<p class="paper"><span class="papertitle">[14] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</span><br />
<span class="authors">A. Dai, A.X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner</span><br />
<span class="journal">Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017</span></p>

<p class="paper"><span class="papertitle">[15]  CARLA: An Open Urban Driving Simulator</span><br />
<span class="authors">A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun</span><br />
<span class="journal">1–16, Proceedings of the 1st Annual Conference on Robot Learning, 2017</span></p>

<p class="paper"><span class="papertitle">[16] SceneNN: A Scene Meshes Dataset with aNNotations</span><br />
<span class="authors">B.S. Hua, Q.H. Pham, D.T. Nguyen, M.K. Tran, L.F. Yu, and S.K. Yeung</span><br />
<span class="journal">International Conference on 3D Vision (3DV), 2016</span></p>


      </div>
    </div>

    

    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
  </body>
</html>
