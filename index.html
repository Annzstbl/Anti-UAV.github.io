
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Anti-UAV</title>
  <meta name="description" content="Catch UAVs that Want to Spy You ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Anti-UAV"/>
  <meta property="og:url" content="https://Anti-UAV.github.io/"/>
  <meta property="og:description" content="Catch UAVs that Want to Spy You ---"/>
  <meta property="og:site_name" content="Anti-UAV"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="/css/main.css" media="screen,projection">
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Call for Papers</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


<div class="container">
  <div class="page-content">
          <p><br /></p>
		  
<div class="row">
  <div class="col-xs-12">
    <center><h1>Catch UAVs that Want to Spy You:</h1></center>
    <center><h2>Detection and Tracking of Unmanned Aerial Vehicle (UAV) in the Wild and the 1<sup>st</sup> Anti-UAV Challenge</h2></center>
    <br />
  </div>
</div>

<hr />

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Main Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
	  <thead>
        <tr>
          <th>Proposers’ names</th>
          <th>Titles</th>
          <th>Affiliations</th>
          <th>Primary contact email</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Jian Zhao</td>
          <td>Assistant Professor</td>
          <td>Institute of North Electronic Equipment</td>
          <td><a href="mailto:zhaojian90@u.nus.edu">zhaojian90@u.nus.edu</a></td>
        </tr>
        <tr>
          <td>Qiang Wang</td>
          <td>Ph.D. Candidate</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="mailto:qiang.wang@nlpr.ia.ac.cn">qiang.wang@nlpr.ia.ac.cn</a></td>
		</tr>
        <tr>
          <td>Junliang Xing</td>
          <td>Professor</td>
          <td>Chinese Academy of Sciences</td>
          <td><a href="mailto:jlxing@nlpr.ia.ac.cn">jlxing@nlpr.ia.ac.cn</a></td>
		</tr>
        <tr>
          <td>Guibo Zhu</td>
          <td>Associate Professor</td>
		  <td>Chinese Academy of Sciences</td>
          <td><a href="mailto:gbzhu@nlpr.ia.ac.cn">gbzhu@nlpr.ia.ac.cn</a></td>
        </tr>
        <tr>
          <td>Xiaopeng Hong</td>
          <td>Distinguished Research Fellow</td>
		  <td>Xi’an Jiaotong University</td>
          <td><a href="mailto:hongxiaopeng@ieee.org">hongxiaopeng@ieee.org</a></td>
        </tr>
        <tr>
          <td>Sheng Mei Shen</td>
          <td>Chief Scientist and Managing Director</td>
		  <td>Pensees Singapore</td>
          <td><a href="mailto:jane.shen@pensees.ai">jane.shen@pensees.ai</a></td>
        </tr>
        <tr>
          <td>Jiashi Feng</td>
          <td>Assistant Professor</td>
		  <td>National University of Singapore</td>
          <td><a href="mailto:elefjia@nus.edu.sg">elefjia@nus.edu.sg</a></td>
        </tr>
	  </tbody>
    </table>
  </div>
</div>


<div class="row">
  <div class="col-xs-12">
    <h2>Topics of interest</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The submissions are expected to deal with visual perception and processing tasks which include but are not limited to:
    </p>
    <ul>
      <li>Applications of computer vision on UAVs</li>
      <li>Strategies for searching of UAVs based on NIR and/or VIS data</li>
      <li>Spectrum sensing techniques for UAVs detection</li>
      <li>Localization and open-set identification of UAVs</li>
      <li>Scene understanding for UAVs</li>
      <li>Small/tiny object detection and tracking techniques</li>
      <li>Fine-grained object recognition</li>
      <li>Real-time deep learning inference</li>
      <li>Infrared image and video analysis</li>
      <li>Multimodal fusion techniques</li>
    </ul>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <h2>Background and experience that makes the proposers well suited for organizing the workshop</h2>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Jian Zhao</b> is currently an assistant professor with Institute of North Electronic Equipment, Beijing, China. He received his Ph.D. degree from National University of Singapore (NUS) in 2019 under the supervision of Assist. Prof. Jiashi Feng, Assoc. Prof. Shuicheng Yan, and Prof. Hengzhu Liu. He has published cutting-edge papers on unconstrained/large-scale/low-shot face verification/identification and human parsing as the first author (including the following conferences and journals: NIPS, CVPR, ECCV, IJCAI, AAAI, ACM MM, BMVC; T-PAMI, IJCV). He has won the Lee Hwee Kuan Award (Gold Award) on PREMIA 2019 as the first author. He has won the “Best Student Paper Award” on ACM MM 2018 as the first author. He has won the top-3 awards several times on world-wide competitions on face recognition, human parsing and pose estimation as the first author. His main research interests include deep learning, pattern recognition, computer vision and multimedia. He and his collaborators has also successfully organized the 2nd Look Into Person (LIP) workshop and challenge on CVPR 2018.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Mr. Qiang Wang</b> is currently a Ph.D. candidate in National Laboratory of Pattern Recognition at the Institute of Automation, Chinese Academic of Sciences, advised by Prof. Weiming Hu. His research interest is mainly computer vision and video object tracking, particularly single object tracking, video object segmentation. He won the VOT2018 real-time challenge. He has participated in the program committee of VOT2018 and VOT2019.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Junliang Xing</b> is currently a professor with the Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China. He received his B.E. degrees in Computer Science and Technology as well as Applied Mathematics from Xi'an Jiaotong University in 2007, and his Ph.D. degree in Computer Science and Technology from Tsinghua University, 2012. He has published over 100 papers in peer-reviewed international conferences like ICCV, CVPR, ECCV, ACM Multimedia, AAAI, IJCAI, and journals like TPAMI, IJCV, TIP, PR. He has translated two books in computer vision and wrote one book on deep learning. Dr. Xing was the recipient of Google Ph.D. Fellowship in 2011, the Best Paper Award of ACM International Conference on Multimedia in 2013, and the champions of many international AI technical competitions in face recognition, pose estimation, etc. His research areas lie in computer vision and machine learning, with a main focus on computer vision problems related to human face and body, and computer gaming problems using deep reinforcement learning models.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Guibo Zhu</b> is currently an associate professor with the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China. He received the B.E. degree from Wuhan University, Wuhan, China, in 2009, the Ph.D. degree from the University of Chinese Academy of Sciences, Beijing, China, in 2016. He has published over 10 papers as the first author in international conferences like AAAI, IJCAI, BMVC, and journals like T-IP, T-NNLS, T-Cyber, CVIU. He has won two champions of domestic AI technical competitions in single visual tracking and video understanding as the first author. His research interests include computer vision, pattern recognition, and machine learning, especially paying attention on single object tracking, object detection, video understanding and meta-learning.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Xiaopeng Hong</b> is currently a distinguished research fellow at Xi’an Jiaotong University, PRC. He had been a Docent with the Center for Machine Vision and Signal Analysis, University of Oulu, Finland, where he had been a senior researcher from 2011 to 2019. Xiaopeng has been a PI of an Infotech Oulu Postdoctoral funding project and the project manager of an Academy of Finland ICT 2023 funding project (also the only co-writer of that proposal). He has (co-)authored over 40 articles in peer-reviewed journals and conferences such as IEEE T-PAMI, IEEE T-IP, CVPR, ICCV, and IJCAI. His research about micro-expression analysis has been reported by International media like MIT Technology Review and Daily Mail. Xiaopeng has served as a reviewer for a few top-tier journals and conferences and has been ranked as an ‘Outstanding Reviewer’ for two Elsevier journals: Pattern Recognition and Neurocomputing. His current research interests include visual surveillance and micro-expression analysis, etc.
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Mrs. Shengmei Shen</b>  received the Graduate and Master degrees from Xidian University, Xi’an, China. She is currently working as Chief Scientist and Managing Director at Pensees Singapore leading AI technology development especially in computer vision and deep learning for smart city including surveillance, smart community, smart manufactory, robotics and other applications with smart innovation and solution. She had been working as an assistant director in Panasonic R&D Center Singapore since 1992 with 300 patents filed, actively involved in image recognition and sensing technology development, machine vision as well as audio–visual compression for B2C and B2B business. Under her leadership and technology vision with strong business sense, she has built up several core competent areas as a strong player in AI development and deployment, to provide the technology and service to company’s products and business. 
    </p>
  </div>
</div>

<div class="row">
  <div class="col-md-12">
    <p>
      <b>Dr. Jiashi Feng</b> is currently an assistant professor with the Department of Electrical and Computer Engineering at National University of Singapore. He received his Ph.D. degree from NUS in 2014. Before joining NUS, he was a postdoc researcher in the EECS department and ICSI at the University of California, Berkeley, working with Trevor Darrell. His research areas include computer vision, machine learning and deep learning. He received the best technical demo award from ACM MM 2012 and best paper award from TASK-CV ICCV 2015.
    </p>
  </div>
</div>


<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Description of the 1<sup>st</sup> Anti-UAV Challenge</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Recently, unmanned aerial vehicle (UAV) is growing rapidly in a wide range of consumer communications and networks with their autonomy, flexibility, and a broad range of application domains. UAV applications offer possible civil and public domain applications in which single or multiple UAVs may be used. At the same time, we also need to be aware of the potential threat to our lives caused by UAV intrusion. Earlier this year, multiple instances of drone sightings halted air traffic at airports, leading to significant economic losses for airlines.
	</p>
    <p>
      This workshop focuses on state-of-the-art anti-UAV systems in a bid to safeguard flights. Historically, radar is certainly a very mature technology for detecting traditional incoming airborne threats. However, today, these small drones are challenging for radar to accurately and reliably detect. It is due to the combination of a very small radar cross section and the erratic flight paths of these UAV drones that make trustworthy detection almost impossible. Indeed, how to use computer vision algorithms to perceive UAVs is a crucial part of the whole defense system.
    </p>
    <p>
      The current computer vision research for UAV lacks a high-quality benchmark in dynamic environments. To mitigate this gap, this workshop presents a benchmark dataset and evaluation methodology for the area of detecting and tracking UAVs. The dataset consists of sixty high quality, Full HD video sequences (both RGB and Thermal Infrared), spanning multiple occurrences of multi-scale UAVs. This workshop also encourages participants to establish approaches to fully automatic detection and tracking of UAVs in videos. Although solutions can be derived from the off-the-shelf detection and tracking algorithms, the training and customizations of them for UAVs pose unique challenges. Specifically, how to detect and track fast-moving drones in noisy (e.g., occlusion by cloud and buildings, and fake targets like kites, balloons, birds, etc.) environments and how to use infrared images to locate UAVs in total darkness are not explored enough.
    </p>
    <p>
      This workshop will bring together academic and industrial experts in the field of UAVs to discuss the techniques and applications of tracking UAVs. Participants are invited to submit their original contributions, surveys, and case studies that address the works of UAV’s detection and tracking issues.
    </p>
  </div>
</div>

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="1.png" />
  </div>
</div>

<p><br /></p>


<div class="row">
  <div class="col-xs-12">
    <h2>
      Rough program outline (including preference for half- or full-day event, estimated numbers of orals, posters, invited talks and conjunct competition details)
    </h2>
    <ul>
      <li>We prefer a <b>half-day</b> event for this workshop.</li>
      <li>Anti-UAV challenge: as part of the workshop, we will host the 1<sup>st</sup> challenge on joint detection and tracking for UAVs. The dataset for the challenge will be released on April 15th, 2020. The proposed dataset will consist of sixty high quality, Full HD video sequences (both RGB and Thermal Infrared), spanning multiple occurrences of multi-scale UAVs. We will build a challenge server on CodaLab which will be open for submission soon afterwards.</li>
      <li>Expect 30 paper submissions, and finally accept 2 oral papers for all winning entries of this challenges), 10 poster papers.</li>
      <li>Tentative program committee: Jian Zhao (Assistant Professor, North Electronic Equipment), Qiang Wang (Ph.D. candidate, CASIA), Junliang Xing (Professor, CASIA), Guibo Zhu (Associate Professor, CASIA), Xiaopeng Hong (Distinguished Research Fellow, Xi’an Jiaotong University), Shengmei Shen (Chief Scientist and Managing Director, Pensees Singapore), Jiashi Feng (Assistant Professor, NUS).</li>
      <li>Program outline at a glance</li>
    </ul>
    <p>
      <table class="table table-striped">
      <tbody>
        <tr>
          <td>08:30-08:40</td>
          <td>Opening remarks and welcome</td>
        </tr>
        <tr>
          <td>08:40-09:10</td>
          <td>The Anti-UAV challenge introduction and results</td>
        </tr>
        <tr>
          <td>09:10-09:25</td>
          <td>Oral talk 1: Winner of UAV detection challenge</td>
        </tr>
        <tr>
          <td>09:25-09:55</td>
          <td>Invited talk 1: Bernard Ghanem, Associate Professor at KAUST</td>
        </tr>
        <tr>
          <td>09:55-10:25</td>
          <td>Poster session and coffee break</td>
        </tr>
        <tr>
          <td>10:25-10:55</td>
          <td>Invited talk 2: Haibing Lin, Professor at University of California, Merced</td>
        </tr>
        <tr>
          <td>10:55-11:10</td>
          <td>Oral talk 2: Winner of UAV tracking challenge</td>
        </tr>
        <tr>
          <td>11:10-11:40</td>
          <td>Invited talk 3: Ming-Hsuan Yang, Professor at University of California, Merced</td>
        </tr>
        <tr>
          <td>11:40-12:00</td>
          <td>Awards & Future Plans</td>
        </tr>
      </tbody>
      </table>
    </p>
  </div>
</div>
<p><br /></p>

<p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <h2>Names and bios of any invited speakers and indication of whether they have agreed to speak</h2>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="p1.png" />
    <p>
	  <b>Invited speaker 1: Bernard Ghanem, Associate Professor, KAUST (Confirmed)</b>
    </p>
	<p>
	  Biography: Bernard Ghanem is currently an Associate Professor at King Abdullah University of Science and Technology (KAUST), in the Visual Computing Center (VCC). I lead the Image and Video Understanding Lab (IVUL) at KAUST. He is also an adjunct Senior Research Scientist at the Advanced Sciences Digital Center (ADSC) of Illinois in Singapore, where he is PI on the Semantic Analysis of Video project . He is involved in several interesting projects that focus on exploiting techniques in computer vision and machine learning for real-world applications including semantic sports video analysis, large-scale activity recognition/detection, and real-time crowd analysis.
	</p>
	<p>
	  He was a graduate research assistant at the Computer Vision and Robotics Lab (CVRL) at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign . His advisor was Professor Narendra Ahuja, who heads CVRL.
	</p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="p2.png" />
    <p>
	  <b>Invited speaker 2: Haibin Lin, Professor, Stony Brook University  (Confirmed)</b>
    </p>
	<p>
      Biography: Haibin Ling received B.S. and M.S. from Peking University in 1997 and 2000, respectively, and Ph.D. from University of Maryland in 2006. From 2000 to 2001, he was an assistant researcher at Microsoft Research Asia; from 2006 to 2007, he worked as a postdoctoral scientist at UCLA; from 2007-2008, he worked for Siemens Corporate Research as a research scientist; and from 2008 to 2019, he was a faculty member of the Department of Computer Sciences for Temple University. In fall 2019, he joined the Department of Computer Science of Stony Brook University, where he is now a SUNY Empire Innovation Professor. His research interests include computer vision, augmented reality, medical image analysis, visual privacy protection, and human computer interaction. He received Best Student Paper Award of ACM UIST in 2003 and NSF CAREER Award in 2014. He serves as associate editors for IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), Pattern Recognition (PR), and Computer Vision and Image Understanding (CVIU). He also serves as Area Chairs for CVPR 2014, 2016, 2019 and 2020.
	</p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="p3.png" />
    <p>
	  <b>Invited speaker 3: Ming-Hsuan Yang, Professor, University of California, Merced. (Confirmed)</b>
    </p>
	<p>
	  Biography: Ming-Hsuan Yang is a Professor in Electrical Engineering and Computer Science at University of California, Merced. He received the PhD degree in computer science from the University of Illinois at Urbana-Champaign in 2000. He studied computer science and power mechanical engineering at the National Tsing-Hua University, Taiwan; computer science and brain theory at the University of Southern California; and artificial intelligence and operations research at the University of Texas at Austin. He was a senior research scientist at the Honda Research Institute (formerly Honda Fundamental Research Labs) working on vision problems related to humanoid robots. In 1999, he received the Ray Ozzie fellowship for his research work. His research interests include computer vision, pattern recognition, artificial intelligence, robotics, and machine learning.
	</p>
  </div>
</div>
<p><br /></p>


<div class="row">
  <div class="col-md-12">
    <p>
	  <b>Anticipated target audience as well as expected number of attendees</b>
    </p>
	<p>
	  This workshop targets at researchers and students who are working on object perception, including detection, tracking, motion trend prediction, object re-identification, object recognition/verification, and other related areas. In total 150 attendees are reasonably expected.
	</p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Description of relevance and viability</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Developing solutions to detection and tracking objects in the wild scenarios, regarded as one of the most fundamental problems in computer vision, could have a crucial impact in many industrial application domains. The goal of this workshop is to allow researchers from the fields of object detection, visual tracking and other disciplines to present their progress, communication and co-develop novel ideas that potentially shape the future of this area and further advance the performance and applicability of correspondingly built systems in real-world conditions.
    </p>
    <p>
      Regarding the viability of this workshop, the topic of this workshop is attractive and active. It is very possible that many active researchers would like to attend this workshop (actually the expected number of attendees is 100 from a conservative estimation based on the past publication record on related topics). It is related to yet still clearly different from past workshops as explained below. In addition, we have got confirmation from many renowned professors and researchers in this area and they are either glad to give a keynote speech (as listed in the program) or kindly offer help. We believe this workshop will be a very successful one and it will indeed benefit the progress of this research area significantly. 
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Description of how this proposal relates to previous workshops at CVPR/ICCV/ECCV (be as specific as possible)</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>ICCV2017 workshop:<ul><li>1st International Workshop on Vision for UAVs</li></ul></li>
      <li>CVPR2018 workshop:<ul><li>Computer Vision for UAVs</li></ul></li>
      <li>ECCV2018 workshop:<ul><li>2nd International Workshop on Computer Vision for UAVs</li><li>Vision Meets Drone: A Challenge</li></ul></li>
      <li>CVPR2019 workshop:<ul><li>3rd International Workshop on Vision for UAVs</li></ul></li>
      <li>ICCV2019 workshop:<ul><li>Vision Meets Drones 2019: A Challenge</li></ul></li>
    </ul>
    <p>
      With the increasing popularity of UAVs, a large number of workshops about UAVs have been held (listed above). They mainly focus on visual tracking, SLAM, depth perception from drone perspective. Our workshop takes a different perspective, making UAVs as tracking targets, and provides a large-scale dataset to promote deep network learning for UAVs. In addition, the proposed workshop also aims at tiny object detection and tracking in the wild which is more challenging, more practical, and more useful for real applications. Thus our workshop will bridge the needs of industry and research in academia, and may accelerate the process of these computer vision technologies being used in real applications.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Challenge open</td>
          <td>February 15th 2020</td>
        </tr>
        <tr>
          <td>Results Submission</td>
          <td>March 15st 2020 (24:00 CET - midnight) -> April 9th 2020 (24:00 CET - midnight)</td>
        </tr>
        <tr>
          <td>Paper Submission</td>
          <td>March 15th 2020 -> April 15th 2020 (24:00 CET - midnight)</td>
        </tr>
        <tr>
          <td>Notification</td>
          <td>May 8th 2020</td>
        </tr>
        <tr>
          <td>Camera-ready</td>
          <td>May 16th 2020</td>
        </tr>
        <tr>
          <td>Workshop</td>
          <td>June 16th 2020</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>


<div class="row">
  <div class="col-xs-12">
    <h2>Any special space or equipment requests</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We will need a digital projector (for presenters) and a flipchart with pens (for publicly recording small-group critiques).
    </p>
  </div>
</div>
<p><br /></p>

      </div>
    </div>

    

    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
  </body>
</html>
